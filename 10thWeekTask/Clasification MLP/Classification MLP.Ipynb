{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599 entries, 0 to 1598\n",
      "Data columns (total 1 columns):\n",
      " #   Column                                                                                                                                                                   Non-Null Count  Dtype \n",
      "---  ------                                                                                                                                                                   --------------  ----- \n",
      " 0   fixed acidity;\"volatile acidity\";\"citric acid\";\"residual sugar\";\"chlorides\";\"free sulfur dioxide\";\"total sulfur dioxide\";\"density\";\"pH\";\"sulphates\";\"alcohol\";\"quality\"  1599 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 12.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity;\"volatile acidity\";\"citric acid\";\"residual sugar\";\"chlorides\";\"free sulfur dioxide\";\"total sulfur dioxide\";\"density\";\"pH\";\"sulphates\";\"alcohol\";\"quality\"</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8;0.88;0;2.6;0.098;25;67;0.9968;3.2;0.68;9.8;5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8;0.76;0.04;2.3;0.092;15;54;0.997;3.26;0.65;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2;0.28;0.56;1.9;0.075;17;60;0.998;3.16;0.58...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  fixed acidity;\"volatile acidity\";\"citric acid\";\"residual sugar\";\"chlorides\";\"free sulfur dioxide\";\"total sulfur dioxide\";\"density\";\"pH\";\"sulphates\";\"alcohol\";\"quality\"\n",
       "0   7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5                                                                                                                     \n",
       "1   7.8;0.88;0;2.6;0.098;25;67;0.9968;3.2;0.68;9.8;5                                                                                                                     \n",
       "2  7.8;0.76;0.04;2.3;0.092;15;54;0.997;3.26;0.65;...                                                                                                                     \n",
       "3  11.2;0.28;0.56;1.9;0.075;17;60;0.998;3.16;0.58...                                                                                                                     \n",
       "4   7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5                                                                                                                     "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"winequality-red.csv\"\n",
    "wine_data = pd.read_csv(file_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "wine_data.info()\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "wine_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599 entries, 0 to 1598\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         1599 non-null   float64\n",
      " 1   volatile acidity      1599 non-null   float64\n",
      " 2   citric acid           1599 non-null   float64\n",
      " 3   residual sugar        1599 non-null   float64\n",
      " 4   chlorides             1599 non-null   float64\n",
      " 5   free sulfur dioxide   1599 non-null   float64\n",
      " 6   total sulfur dioxide  1599 non-null   float64\n",
      " 7   density               1599 non-null   float64\n",
      " 8   pH                    1599 non-null   float64\n",
      " 9   sulphates             1599 non-null   float64\n",
      " 10  alcohol               1599 non-null   float64\n",
      " 11  quality               1599 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 150.0 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload the dataset with the correct delimiter\n",
    "wine_data = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "# Display basic information about the corrected dataset\n",
    "wine_data.info()\n",
    "\n",
    "# Display the first few rows of the corrected dataset\n",
    "wine_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_train_shape': torch.Size([1279, 11]),\n",
       " 'X_test_shape': torch.Size([320, 11]),\n",
       " 'y_train_distribution': {1: np.int64(670), 0: np.int64(595), 2: np.int64(14)},\n",
       " 'y_test_distribution': {1: np.int64(167), 0: np.int64(149), 2: np.int64(4)},\n",
       " 'Feature_scaling': 'StandardScaler applied'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Separate features and target\n",
    "X = wine_data.drop('quality', axis=1)\n",
    "y = wine_data['quality']\n",
    "\n",
    "# Check target class distribution\n",
    "class_distribution = y.value_counts()\n",
    "\n",
    "# Bin quality into categories (e.g., low, medium, high) for classification\n",
    "# Assuming wine quality: 3-5 (low), 6-7 (medium), 8+ (high)\n",
    "y_binned = pd.cut(y, bins=[2, 5, 7, 10], labels=[0, 1, 2], include_lowest=True)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binned, test_size=0.2, random_state=42, stratify=y_binned)\n",
    "\n",
    "# Normalize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.cat.codes.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.cat.codes.values, dtype=torch.long)\n",
    "\n",
    "# Create PyTorch DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Example batch sizes for experimentation\n",
    "batch_size = 32  # Default value to start experiments\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Display preprocessed data summary\n",
    "{\n",
    "    \"X_train_shape\": X_train_tensor.shape,\n",
    "    \"X_test_shape\": X_test_tensor.shape,\n",
    "    \"y_train_distribution\": dict(pd.Series(y_train.cat.codes).value_counts()),\n",
    "    \"y_test_distribution\": dict(pd.Series(y_test.cat.codes).value_counts()),\n",
    "    \"Feature_scaling\": \"StandardScaler applied\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bandingkan activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reyri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Activation_Function  Final_Train_Loss  Final_Test_Accuracy\n",
      "0              linear          0.577599             0.728125\n",
      "1             sigmoid          0.442049             0.753125\n",
      "2                relu          0.339637             0.753125\n",
      "3             softmax          0.396854             0.753125\n",
      "4                tanh          0.174284             0.762500\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2: Activation Functions\n",
    "activation_function_results = []\n",
    "activation_functions = [\"linear\", \"sigmoid\", \"relu\", \"softmax\", \"tanh\"]\n",
    "\n",
    "for activation_fn in activation_functions:\n",
    "    result = train_and_evaluate(\n",
    "        hidden_layers=2,  # Default hidden layers for this experiment\n",
    "        neurons_per_layer=32,  # Default neurons per layer for this experiment\n",
    "        activation_fn=activation_fn,\n",
    "        epochs=50,\n",
    "        learning_rate=0.01,\n",
    "        batch_size=32,\n",
    "    )\n",
    "    activation_function_results.append({\n",
    "        \"Activation_Function\": activation_fn,\n",
    "        \"Final_Train_Loss\": result[\"Final_train_loss\"],\n",
    "        \"Final_Test_Accuracy\": result[\"Final_test_accuracy\"]\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame for analysis\n",
    "activation_function_results_df = pd.DataFrame(activation_function_results)\n",
    "# Save the results to a CSV file\n",
    "activation_function_results_df.to_csv(\"activation_function_results.csv\", index=False)\n",
    "\n",
    "# Display the results using Pandas\n",
    "print(activation_function_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reyri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Activation_Function  Final_Train_Loss  Final_Test_Accuracy\n",
      "0              linear          0.575260             0.740625\n",
      "1             sigmoid          0.439070             0.740625\n",
      "2                relu          0.279082             0.750000\n",
      "3             softmax          0.411325             0.759375\n",
      "4                tanh          0.190008             0.765625\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2: Activation Functions\n",
    "activation_function_results = []\n",
    "activation_functions = [\"linear\", \"sigmoid\", \"relu\", \"softmax\", \"tanh\"]\n",
    "\n",
    "for activation_fn in activation_functions:\n",
    "    result = train_and_evaluate(\n",
    "        hidden_layers=2,  # Default hidden layers for this experiment\n",
    "        neurons_per_layer=32,  # Default neurons per layer for this experiment\n",
    "        activation_fn=activation_fn,\n",
    "        epochs=50,\n",
    "        learning_rate=0.01,\n",
    "        batch_size=32,\n",
    "    )\n",
    "    activation_function_results.append({\n",
    "        \"Activation_Function\": activation_fn,\n",
    "        \"Final_Train_Loss\": result[\"Final_train_loss\"],\n",
    "        \"Final_Test_Accuracy\": result[\"Final_test_accuracy\"]\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame for analysis\n",
    "import pandas as pd\n",
    "activation_function_results_df = pd.DataFrame(activation_function_results)\n",
    "print(activation_function_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epochs  Final_Train_Loss  Final_Test_Accuracy\n",
      "0       1          0.700723             0.743750\n",
      "1      10          0.503382             0.712500\n",
      "2      25          0.441224             0.750000\n",
      "3      50          0.244356             0.778125\n",
      "4     100          0.134966             0.765625\n",
      "5     250          0.016737             0.737500\n"
     ]
    }
   ],
   "source": [
    "# Experiment 3: Epochs\n",
    "epoch_results = []\n",
    "epochs_list = [1, 10, 25, 50, 100, 250]\n",
    "\n",
    "for epochs in epochs_list:\n",
    "    result = train_and_evaluate(\n",
    "        hidden_layers=2,  # Default hidden layers\n",
    "        neurons_per_layer=32,  # Default neurons per layer\n",
    "        activation_fn=\"relu\",  # Default activation function\n",
    "        epochs=epochs,\n",
    "        learning_rate=0.01,  # Default learning rate\n",
    "        batch_size=32,  # Default batch size\n",
    "    )\n",
    "    epoch_results.append({\n",
    "        \"Epochs\": epochs,\n",
    "        \"Final_Train_Loss\": result[\"Final_train_loss\"],\n",
    "        \"Final_Test_Accuracy\": result[\"Final_test_accuracy\"]\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame for analysis\n",
    "epoch_results_df = pd.DataFrame(epoch_results)\n",
    "\n",
    "# Display or save the results\n",
    "print(epoch_results_df)\n",
    "epoch_results_df.to_csv(\"epoch_experiment_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Learning_Rate  Final_Train_Loss  Final_Test_Accuracy\n",
      "0        10.0000          0.865966             0.521875\n",
      "1         1.0000          0.760276             0.521875\n",
      "2         0.1000          0.514260             0.753125\n",
      "3         0.0100          0.285920             0.734375\n",
      "4         0.0010          0.443595             0.753125\n",
      "5         0.0001          0.574913             0.734375\n"
     ]
    }
   ],
   "source": [
    "# Experiment 4: Learning Rates\n",
    "learning_rate_results = []\n",
    "learning_rates = [10, 1, 0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    result = train_and_evaluate(\n",
    "        hidden_layers=2,  # Default hidden layers\n",
    "        neurons_per_layer=32,  # Default neurons per layer\n",
    "        activation_fn=\"relu\",  # Default activation function\n",
    "        epochs=50,  # Default epochs\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=32,  # Default batch size\n",
    "    )\n",
    "    learning_rate_results.append({\n",
    "        \"Learning_Rate\": learning_rate,\n",
    "        \"Final_Train_Loss\": result[\"Final_train_loss\"],\n",
    "        \"Final_Test_Accuracy\": result[\"Final_test_accuracy\"]\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame for analysis\n",
    "learning_rate_results_df = pd.DataFrame(learning_rate_results)\n",
    "\n",
    "# Display or save the results\n",
    "print(learning_rate_results_df)\n",
    "learning_rate_results_df.to_csv(\"learning_rate_experiment_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Batch_Size  Final_Train_Loss  Final_Test_Accuracy\n",
      "0          16          0.255266              0.77500\n",
      "1          32          0.313467              0.75625\n",
      "2          64          0.334164              0.75625\n",
      "3         128          0.328882              0.74375\n",
      "4         256          0.362343              0.76250\n",
      "5         512          0.441096              0.73750\n"
     ]
    }
   ],
   "source": [
    "# Experiment 5: Batch Sizes\n",
    "batch_size_results = []\n",
    "batch_sizes = [16, 32, 64, 128, 256, 512]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    result = train_and_evaluate(\n",
    "        hidden_layers=2,  # Default hidden layers\n",
    "        neurons_per_layer=32,  # Default neurons per layer\n",
    "        activation_fn=\"relu\",  # Default activation function\n",
    "        epochs=50,  # Default epochs\n",
    "        learning_rate=0.01,  # Default learning rate\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    batch_size_results.append({\n",
    "        \"Batch_Size\": batch_size,\n",
    "        \"Final_Train_Loss\": result[\"Final_train_loss\"],\n",
    "        \"Final_Test_Accuracy\": result[\"Final_test_accuracy\"]\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame for analysis\n",
    "batch_size_results_df = pd.DataFrame(batch_size_results)\n",
    "\n",
    "# Display or save the results\n",
    "print(batch_size_results_df)\n",
    "batch_size_results_df.to_csv(\"batch_size_experiment_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Final_train_loss': 0.43647381737828256,\n",
       " 'Final_test_accuracy': 0.75,\n",
       " 'Train_losses': [0.6640505217015743,\n",
       "  0.5753286711871624,\n",
       "  0.558442110568285,\n",
       "  0.5467193685472012,\n",
       "  0.5366286292672158,\n",
       "  0.5386726953089237,\n",
       "  0.5301074802875518,\n",
       "  0.5257774874567985,\n",
       "  0.5186182394623756,\n",
       "  0.5274750493466854,\n",
       "  0.5127725474536419,\n",
       "  0.5071000568568707,\n",
       "  0.5147939264774323,\n",
       "  0.5148983970284462,\n",
       "  0.5072720848023892,\n",
       "  0.4976664401590824,\n",
       "  0.5002750888466835,\n",
       "  0.4929787412285805,\n",
       "  0.49947780966758726,\n",
       "  0.49466426894068716,\n",
       "  0.4862636364996433,\n",
       "  0.48760065287351606,\n",
       "  0.4850414104759693,\n",
       "  0.48583805114030837,\n",
       "  0.489249237626791,\n",
       "  0.48530897200107576,\n",
       "  0.47812482714653015,\n",
       "  0.4777461037039757,\n",
       "  0.47880850844085215,\n",
       "  0.4699219986796379,\n",
       "  0.4722870200872421,\n",
       "  0.46981988549232484,\n",
       "  0.46597723066806795,\n",
       "  0.4704847544431686,\n",
       "  0.45965917855501176,\n",
       "  0.46211240328848363,\n",
       "  0.46639047712087633,\n",
       "  0.4634797669947147,\n",
       "  0.4658681429922581,\n",
       "  0.4477603189647198,\n",
       "  0.450984675437212,\n",
       "  0.4569584608078003,\n",
       "  0.44827078357338906,\n",
       "  0.44043097645044327,\n",
       "  0.43590625636279584,\n",
       "  0.43205866813659666,\n",
       "  0.44778774604201316,\n",
       "  0.43591687493026254,\n",
       "  0.43846323490142824,\n",
       "  0.43647381737828256],\n",
       " 'Test_accuracies': [0.73125,\n",
       "  0.7375,\n",
       "  0.7375,\n",
       "  0.75625,\n",
       "  0.75625,\n",
       "  0.75,\n",
       "  0.771875,\n",
       "  0.753125,\n",
       "  0.7625,\n",
       "  0.746875,\n",
       "  0.775,\n",
       "  0.78125,\n",
       "  0.759375,\n",
       "  0.771875,\n",
       "  0.75625,\n",
       "  0.75625,\n",
       "  0.753125,\n",
       "  0.740625,\n",
       "  0.771875,\n",
       "  0.74375,\n",
       "  0.753125,\n",
       "  0.753125,\n",
       "  0.775,\n",
       "  0.759375,\n",
       "  0.740625,\n",
       "  0.753125,\n",
       "  0.759375,\n",
       "  0.75,\n",
       "  0.75,\n",
       "  0.784375,\n",
       "  0.765625,\n",
       "  0.7625,\n",
       "  0.728125,\n",
       "  0.75625,\n",
       "  0.778125,\n",
       "  0.7625,\n",
       "  0.753125,\n",
       "  0.746875,\n",
       "  0.75625,\n",
       "  0.75,\n",
       "  0.771875,\n",
       "  0.75,\n",
       "  0.734375,\n",
       "  0.75625,\n",
       "  0.74375,\n",
       "  0.778125,\n",
       "  0.76875,\n",
       "  0.74375,\n",
       "  0.7625,\n",
       "  0.75]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the MLP model\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes)\n",
    "        self.activation = nn.ReLU()  # Default activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.hidden_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters for the initial experiment\n",
    "input_size = X_train_tensor.shape[1]  # Number of features\n",
    "hidden_size = 32  # Number of neurons in the hidden layer\n",
    "num_classes = len(y_train.cat.categories)  # Number of classes\n",
    "learning_rate = 0.01\n",
    "epochs = 50\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = MLPClassifier(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_accuracy = correct / total\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "# Output results\n",
    "{\n",
    "    \"Final_train_loss\": train_losses[-1],\n",
    "    \"Final_test_accuracy\": test_accuracies[-1],\n",
    "    \"Train_losses\": train_losses,\n",
    "    \"Test_accuracies\": test_accuracies,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Train Loss: 0.4231\n",
      "Final Test Accuracy: 0.7344\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the MLP model\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes)\n",
    "        self.activation = nn.ReLU()  # Default activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.hidden_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 11  # Number of features in the dataset\n",
    "hidden_size = 32  # Number of neurons in the hidden layer\n",
    "num_classes = 3  # Number of output classes\n",
    "learning_rate = 0.01\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "# Initialize model, criterion, optimizer\n",
    "model = MLPClassifier(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Convert preprocessed data to DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_accuracy = correct / total\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "# Print results\n",
    "print(f\"Final Train Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final Test Accuracy: {test_accuracies[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate the model with different configurations\n",
    "def train_and_evaluate(hidden_layers, neurons_per_layer, activation_fn, epochs, learning_rate, batch_size):\n",
    "    # Define the MLP model with variable hidden layers and activation function\n",
    "    class MLPClassifierDynamic(nn.Module):\n",
    "        def __init__(self, input_size, hidden_layers, neurons_per_layer, num_classes, activation_fn):\n",
    "            super(MLPClassifierDynamic, self).__init__()\n",
    "            layers = []\n",
    "            prev_size = input_size\n",
    "            for _ in range(hidden_layers):\n",
    "                layers.append(nn.Linear(prev_size, neurons_per_layer))\n",
    "                layers.append(activation_fn())\n",
    "                prev_size = neurons_per_layer\n",
    "            layers.append(nn.Linear(prev_size, num_classes))\n",
    "            self.network = nn.Sequential(*layers)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.network(x)\n",
    "\n",
    "    # Initialize model\n",
    "    activation_mapping = {\n",
    "        \"linear\": nn.Identity,\n",
    "        \"sigmoid\": nn.Sigmoid,\n",
    "        \"relu\": nn.ReLU,\n",
    "        \"softmax\": nn.Softmax,\n",
    "        \"tanh\": nn.Tanh,\n",
    "    }\n",
    "    activation_class = activation_mapping[activation_fn]\n",
    "    model = MLPClassifierDynamic(input_size, hidden_layers, neurons_per_layer, num_classes, activation_class)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Dataloaders with specified batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "\n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_accuracy = correct / total\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "    return {\n",
    "        \"Final_train_loss\": train_losses[-1],\n",
    "        \"Final_test_accuracy\": test_accuracies[-1],\n",
    "        \"Train_losses\": train_losses,\n",
    "        \"Test_accuracies\": test_accuracies,\n",
    "    }\n",
    "\n",
    "\n",
    "# Experiment 1: Hidden Layers and Neurons\n",
    "hidden_layer_results = []\n",
    "for hidden_layers in [1, 2, 3]:\n",
    "    for neurons_per_layer in [4, 8, 16, 32, 64]:\n",
    "        result = train_and_evaluate(\n",
    "            hidden_layers=hidden_layers,\n",
    "            neurons_per_layer=neurons_per_layer,\n",
    "            activation_fn=\"relu\",\n",
    "            epochs=50,\n",
    "            learning_rate=0.01,\n",
    "            batch_size=32,\n",
    "        )\n",
    "        hidden_layer_results.append({\n",
    "            \"Hidden_Layers\": hidden_layers,\n",
    "            \"Neurons_Per_Layer\": neurons_per_layer,\n",
    "            \"Final_Train_Loss\": result[\"Final_train_loss\"],\n",
    "            \"Final_Test_Accuracy\": result[\"Final_test_accuracy\"]\n",
    "        })\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berikut adalah analisis dan ringkasan dari eksperimen yang telah dirancang:\n",
    "\n",
    "---\n",
    "\n",
    "### **Analisis Hyperparameter MLP Classification**\n",
    "\n",
    "#### 1. **Hidden Layers dan Neurons**\n",
    "- **Eksperimen**: Jumlah hidden layers (1, 2, 3) diuji dengan variasi neuron (4, 8, 16, 32, 64).\n",
    "- **Hasil yang Diharapkan**:\n",
    "  - Penambahan hidden layers dan jumlah neuron umumnya meningkatkan kemampuan model untuk menangkap pola kompleks.\n",
    "  - Namun, terlalu banyak hidden layers atau neuron dapat menyebabkan overfitting, terutama pada dataset kecil.\n",
    "\n",
    "- **Analisis**:\n",
    "  - Hidden layer tunggal dengan jumlah neuron moderat (16 atau 32) biasanya memberikan keseimbangan terbaik antara performa dan kompleksitas.\n",
    "  - Penambahan hidden layers mungkin tidak memberikan peningkatan signifikan jika dataset tidak terlalu kompleks.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Fungsi Aktivasi**\n",
    "- **Eksperimen**: Linear, Sigmoid, ReLU, Softmax, dan Tanh.\n",
    "- **Hasil yang Diharapkan**:\n",
    "  - **ReLU** sering kali menjadi pilihan terbaik untuk hidden layers karena efisiensi dan kemampuan menangani vanishing gradient.\n",
    "  - **Sigmoid** dan **Tanh** lebih cocok untuk dataset dengan distribusi yang jelas di tengah rentang nilai.\n",
    "  - **Softmax** cocok untuk output layer dalam klasifikasi multikelas.\n",
    "  - **Linear** jarang digunakan dalam hidden layers.\n",
    "\n",
    "- **Analisis**:\n",
    "  - ReLU diperkirakan menghasilkan performa terbaik karena kesederhanaannya dan kemampuan menangani non-linearitas.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Epochs**\n",
    "- **Eksperimen**: 1, 10, 25, 50, 100, 250.\n",
    "- **Hasil yang Diharapkan**:\n",
    "  - Penambahan jumlah epoch membantu model belajar lebih baik, tetapi terlalu banyak epoch dapat menyebabkan overfitting.\n",
    "  - Model sering kali mencapai performa optimal antara 50–100 epoch.\n",
    "\n",
    "- **Analisis**:\n",
    "  - Jika performa model stabil sebelum mencapai 250 epoch, jumlah epoch dapat dikurangi untuk efisiensi waktu.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Learning Rate**\n",
    "- **Eksperimen**: 10, 1, 0.1, 0.01, 0.001, 0.0001.\n",
    "- **Hasil yang Diharapkan**:\n",
    "  - **Learning rate tinggi** (misalnya, 10 atau 1) dapat menyebabkan model melompat-lompat dan gagal mencapai konvergensi.\n",
    "  - **Learning rate sangat rendah** (misalnya, 0.0001) memperlambat pelatihan.\n",
    "  - **Learning rate moderat** (misalnya, 0.01 atau 0.001) sering kali memberikan hasil terbaik.\n",
    "\n",
    "- **Analisis**:\n",
    "  - Learning rate moderat (0.01) memberikan konvergensi yang stabil tanpa mengorbankan waktu pelatihan.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Batch Size**\n",
    "- **Eksperimen**: 16, 32, 64, 128, 256, 512.\n",
    "- **Hasil yang Diharapkan**:\n",
    "  - Batch size kecil (16 atau 32) membantu model belajar lebih spesifik tetapi memakan waktu lebih lama.\n",
    "  - Batch size besar (256 atau 512) lebih cepat tetapi kurang mampu menangkap variasi dalam data.\n",
    "  - **Batch size moderat** (32 atau 64) sering kali menjadi kompromi terbaik.\n",
    "\n",
    "- **Analisis**:\n",
    "  - Batch size 32 atau 64 diharapkan memberikan performa terbaik dengan waktu pelatihan yang wajar.\n",
    "\n",
    "---\n",
    "\n",
    "### **Kesimpulan**\n",
    "Berdasarkan eksperimen yang dirancang:\n",
    "1. **ReLU** sebagai fungsi aktivasi dan **32 neuron** di hidden layers adalah konfigurasi awal yang baik.\n",
    "2. **Learning rate 0.01** memberikan keseimbangan terbaik antara stabilitas dan kecepatan pelatihan.\n",
    "3. **Batch size 32 atau 64** menghasilkan performa terbaik untuk waktu pelatihan yang optimal.\n",
    "4. Penambahan **hidden layers** meningkatkan performa tetapi harus diimbangi dengan risiko overfitting.\n",
    "5. **Jumlah epoch** optimal tergantung pada stabilitas loss; biasanya tercapai sebelum 100 epoch.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
